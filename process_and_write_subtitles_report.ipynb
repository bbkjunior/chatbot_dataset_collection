{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import string\n",
    "\n",
    "import progressbar\n",
    "from time import sleep\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "import pymorphy2\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import re\n",
    "\n",
    "from gensim.models.keyedvectors import FastTextKeyedVectors\n",
    "\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "from scipy.spatial import distance\n",
    "import operator\n",
    "\n",
    "import statistics\n",
    "\n",
    "full_punctuation = punctuation + \"–\" + \",\" + \"»\" + \"«\"\n",
    "full_punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_metastring(line):\n",
    "    \"\"\"отбрасываем строки без текста\"\"\"\n",
    "    for char in line:\n",
    "        if char != \" \" and char not in full_punctuation and char.isalpha() :\n",
    "            return line\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def lemmatize(text,morph):\n",
    "\"\"\"лемматизируем строки\"\"\"\n",
    "    lemm_split_text = []\n",
    "    tokens = text.split()\n",
    "    #print(tokens)\n",
    "    \n",
    "    clean_text = []\n",
    "    for word in tokens:\n",
    "        if word not in russian_stopwords and word != \" \":\n",
    "            clean_word = ''\n",
    "            for char in word:\n",
    "                if char not in punctuation:\n",
    "                    clean_word+= char\n",
    "            if(clean_word):\n",
    "                clean_text.append(clean_word)\n",
    "    \n",
    "   # print(clean_text)\n",
    "    \n",
    "    for word in clean_text:\n",
    "        parsed_word = morph.parse(word)[0]\n",
    "        lemma = parsed_word.normal_form\n",
    "        \n",
    "        lemm_split_text.append(lemma)\n",
    "    return ' '.join(lemm_split_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(cleaned_text):\n",
    "    \"\"\"лемматизируем текст\"\"\"\n",
    "    lemm_text = []\n",
    "    bar = progressbar.ProgressBar(maxval=len(cleaned_text),\n",
    "                                  widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "    bar.start()\n",
    "    lines = 0\n",
    "\n",
    "    \n",
    "    for line in cleaned_text:\n",
    "        t = lemmatize(line, morph)\n",
    "        lemm_text.append(t)\n",
    "\n",
    "        lines += 1\n",
    "        bar.update(lines)\n",
    "        sleep(0.1) \n",
    "        \n",
    "    return lemm_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_and_lemm_csv(file, filename):\n",
    "    \"\"\"на вход принимается файл с субтитрами и его название, на выходе получаем датасет с оригинальным и лемматизированным текстом, \n",
    "    разделенным в соответсвтие с моментом произношения\"\"\"\n",
    "    print(\"NOW HANDLING \", file)\n",
    "    \n",
    "    cleaned_text = []\n",
    "    with open(file, \"r\", encoding = \"cp1251\") as sub:\n",
    "        \n",
    "        phrase = []\n",
    "        parse_full_phrase = False\n",
    "        try:\n",
    "            for orig_line in sub.readlines():\n",
    "                line = check_metastring(orig_line)\n",
    "                #print(\"<||\", line,\"||>\")\n",
    "\n",
    "                if not line and phrase:\n",
    "                    output_string = ''\n",
    "                    #print(phrase)\n",
    "                    for p_text in phrase:\n",
    "                        #print(p_text)\n",
    "                        p_text = re.sub(' +', ' ', p_text)\n",
    "                        p_text = p_text.rstrip()\n",
    "                        p_text = p_text.lstrip()\n",
    "                        output_string +=  p_text +' '\n",
    "                    output_string = output_string.rstrip()\n",
    "\n",
    "                    if output_string != '':\n",
    "                        cleaned_text.append(output_string)\n",
    "                    phrase = []\n",
    "                else:\n",
    "                    phrase.append(line)\n",
    "        except:\n",
    "            print(\"looks like encoding error\")\n",
    "            return None, None\n",
    "                \n",
    "    lemm_text = preprocess_text(cleaned_text)\n",
    "    \n",
    "    processed_file = pd.DataFrame(\n",
    "    {'cleaned_text': cleaned_text,\n",
    "     'lemm_text': lemm_text,\n",
    "    })\n",
    "    \n",
    "    filename = filename + \".csv\"\n",
    "    print(\"going to save\", filename)\n",
    "    processed_file.to_csv(filename, sep='\\t', encoding='cp1251', index=False)\n",
    "    return cleaned_text, lemm_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in listdir('./sub/subs_all/'):\n",
    "    if item.endswith(\".srt\"):\n",
    "        print (join('./sub/subs_all', item))\n",
    "        get_clean_and_lemm_csv(join('./sub/subs_all', item), item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
